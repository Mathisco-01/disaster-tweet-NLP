{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "realornotkaggle.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSc1Po-9C3AB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  !pip install tensorflow-gpu==2.1.0\n",
        "except:\n",
        "  print(\"Couldn't install tensorflow-gpu 2.1.0\")\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ZxKCl90Ucv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloads the train and test csv's\n",
        "import os\n",
        "\n",
        "if(os.path.isfile('test.csv') == False): \n",
        "  !wget https://raw.githubusercontent.com/Mathisco-01/disaster-tweet-NLP/master/test.csv\n",
        "else:\n",
        "  print(\"test.csv already exists\")\n",
        "\n",
        "if(os.path.isfile('train.csv') == False): \n",
        "  !wget https://raw.githubusercontent.com/Mathisco-01/disaster-tweet-NLP/master/train.csv\n",
        "else:\n",
        "  print(\"train.csv already exists\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0pfyrkHDghF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "chars_to_remove = list(string.punctuation) + [str(c) for c in range(10)]\n",
        "\n",
        "def remove_garbage(text):\n",
        "  output = ''\n",
        "  split = text.split(\" \")\n",
        "  for word in split:\n",
        "    if \"http\" in word:\n",
        "      word = \"http\"\n",
        "    for char in word:\n",
        "      output += char.lower()\n",
        "    output += \" \"\n",
        "      \n",
        "  return output\n",
        "\n",
        "train['text'] = train['text'].apply(remove_garbage)\n",
        "test['text'] = test['text'].apply(remove_garbage)\n",
        "\n",
        "train_X = train.values[:,3]\n",
        "train_Y = [float(y) for y in train.values[:,4:]]\n",
        "\n",
        "train_X, train_Y = shuffle(train_X, train_Y)\n",
        "\n",
        "test_X = test.values[:, 3]\n",
        "\n",
        "print(\"train_X len: ({},)\".format(len(train_X)))\n",
        "print(train_X[:5])\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"train_Y len: ({},)\".format(len(train_Y)))\n",
        "print(train_Y[:5])\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"test_X len: ({},)\".format(len(test_X)))\n",
        "print(test_X[:5])\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0BuTiP4D7aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_length = 15 #maximum sentence length, shorter sentences will be padded\n",
        "vocab_cutoff = 10 #minimum frequency of a word before it becomes a valid token\n",
        "\n",
        "tokens = {\"<oot>\":0}\n",
        "tokens_to_words = {0:\"<oot>\"}\n",
        "tokens_frequency = {0:0}\n",
        "tokens_length = 1\n",
        "\n",
        "# Build tokenization\n",
        "for i in range(len(train_X)):\n",
        "  sentence = []\n",
        "  for word in train_X[i].split(\" \"):\n",
        "    if word not in tokens:\n",
        "      tokens[word] = tokens_length\n",
        "      tokens_to_words[tokens_length] = word  \n",
        "      tokens_frequency[tokens_length] = 1 \n",
        "      tokens_length += 1\n",
        "    else:\n",
        "      tokens_frequency[tokens[word]] += 1 \n",
        "\n",
        "# Cut off tokens with a frequency less than vocab_cutoff\n",
        "new_tokens = {\"<oot>\":0}\n",
        "new_tokens_length = 1\n",
        "for i in range(tokens_length):\n",
        "  if tokens_frequency[i] >= vocab_cutoff:\n",
        "    new_tokens[tokens_to_words[i]] = new_tokens_length\n",
        "    new_tokens_length += 1\n",
        "    \n",
        "tokens = new_tokens\n",
        "tokens_length = new_tokens_length\n",
        "\n",
        "def tokenize_dataset(dataset):\n",
        "  ds = []\n",
        "  for i in range(len(dataset)):\n",
        "    sentence = []\n",
        "    for word in dataset[i]:\n",
        "      if word in tokens:\n",
        "        sentence.append(tokens[word])\n",
        "      else:\n",
        "        sentence.append(0) # oot\n",
        "\n",
        "    if (len(sentence) > sentence_length):\n",
        "      sentence = sentence[:sentence_length]\n",
        "    else:\n",
        "      sentence += [0] * (sentence_length - len(sentence))\n",
        "\n",
        "    ds.append(sentence)\n",
        "  \n",
        "  return np.array(ds)\n",
        "\n",
        "# Applying tokenization on datasets\n",
        "train_X = tokenize_dataset(train_X)\n",
        "test_X = tokenize_dataset(test_X)\n",
        "\n",
        "print(\"train_X shape: {}\".format(train_X.shape))\n",
        "print(train_X[:3])\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"test_X shape: {}\".format(train_X.shape))\n",
        "print(test_X[:3])\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky2wBmfoHo7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(tokens_length, 100, input_length=sentence_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50)),\n",
        "    tf.keras.layers.Dropout(.1),\n",
        "    tf.keras.layers.Dense(1024, activation='relu'),\n",
        "    tf.keras.layers.Dropout(.1),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(.1),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Xc4b4e9DW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# LOAD THE PRE-TRAINED WEIGHTS\n",
        "#\n",
        "\n",
        "if(os.path.isfile('model.h5') == False): \n",
        "  !wget https://raw.githubusercontent.com/Mathisco-01/disaster-tweet-NLP/master/model.h5\n",
        "  \n",
        "model.load_weights(\"model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4CuuHEpOtjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# OR TRAIN THEM YOURSELF!!\n",
        "#\n",
        "\n",
        "train_X = np.asarray(train_X)\n",
        "train_Y = np.asarray(train_Y)\n",
        "history = model.fit(train_X, train_Y, validation_split=.1, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LejThp05_7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if \n",
        "model.save_weights(\"model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hICVVeD37G9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(test_X)\n",
        "\n",
        "import csv\n",
        "with open(\"testfile.csv\", \"w\") as testfile:\n",
        "  filewriter = csv.writer(testfile)\n",
        "  filewriter.writerow(['id', 'target'])\n",
        "  for i in range(len(test_X)):\n",
        "    filewriter.writerow([test.values[i, 0], str(int(np.round(preds[i])[0]))])\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('testfile.csv')\n",
        "except:\n",
        "  print(\"Couldn't download testfile. You're probably not using a google colab notebook\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}